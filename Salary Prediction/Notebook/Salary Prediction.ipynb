{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de82d7fd",
   "metadata": {},
   "source": [
    "![Anz](https://s3.amazonaws.com/pro.brandkit.io/accounts/anz/asset_files/201181/large_thumb_preview.png?updatedAt=2016-11-17T02:33:44 \"Anz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14d0a8",
   "metadata": {},
   "source": [
    "> **Project Title:** Customer Salary Prediction<br>\n",
    "> **Project Owner:** David Adarkwah<br>\n",
    "> **Email:** davidwyse48@gmail.com<br>\n",
    "> **Github Profile:** [Github](github.com/Adark-Amal)<br>\n",
    "> **LinkedIn Profile:** [LinkedIn](https://www.linkedin.com/in/d-adark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6628eea",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd1eb9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b6b5c",
   "metadata": {},
   "source": [
    "## Table of Contents <a id='mu'></a>\n",
    "\n",
    "* [Business Problem Understanding](#bpu)\n",
    "    * [Problem Statement](#ps)\n",
    "    * [Hypothesis](#hs)\n",
    "    * [Project Goal](#pg)\n",
    "    * [Information Needed](#in)\n",
    "    * [Methodology](#my)\n",
    "* [Data Preparation](#dp)\n",
    "    * [Data Quality Assessment](#dqa)\n",
    "    * [Data Cleaning and Preprocessing](#dcp)\n",
    "* [Exploratory Data Analysis](#eda) \n",
    "* [Statistical Analysis](#sa)\n",
    "    * [Removing Outliers](#ro)\n",
    "    * [Normality Test](#nt)\n",
    "    * [Homogeneity of Variances](#hov)\n",
    "    * [Box-Cos Transformation](#bct)\n",
    "    * [Hypothesis Test](#ht)\n",
    "    * [Effect Size](#es)\n",
    "    * [Confidence Interval](#ci)\n",
    "* [Modeling](#dm)\n",
    "    * [Data Understanding](#du)\n",
    "        * [Descriptive Statistics](#ds)\n",
    "        * [Data Visualization](#dv)\n",
    "    * [Feature Engineering and Selection](#fes)\n",
    "    * [Splitting Dataset](#sd)\n",
    "    * [Algorithm Evaluation](#ae)\n",
    "    * [Hyperparameter Tuning](#pt)\n",
    "    * [Finalizing Model](#fm)\n",
    "    * [Model Understanding](#mdu)\n",
    "    * [Save Model](#sm)\n",
    "* [Conclusion and Recommendation](#cr)\n",
    "* [References](#r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbe8eb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14606c1b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4b3f0",
   "metadata": {},
   "source": [
    "## 1. Business Problem Understanding<a id='bpu'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9d257",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify;\">The first step in approaching a data science problem is problem understanding. This step is very important since it allows us to know the kind of decisions we want to make, the information or data that will be needed to inform those decisions and finally, the kind of analysis that will be used to arrive at those decisions. In a nutshell, developing a mental model of the problem allows us to properly structure potentially relevant information needed to solve the problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5282ad",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401769f0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cae0c9",
   "metadata": {},
   "source": [
    "### 1.1 Problem Statement <a id='ps'></a>\n",
    "\n",
    "ANZ has a synthesised transaction dataset containing 3 months’ worth of transactions for 100 hypothetical customers. It contains purchases, recurring transactions, and salary transactions. Based on this dataset, ANZ will want to understand the behaviours of their customers and how transactions are undertaken by each hypothetical customer and finally, be able to predict the annual salary of their present and potential customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00dee52",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335c25c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bafee",
   "metadata": {},
   "source": [
    "### 1.2 Hypothesis <a id='hs'></a>\n",
    "\n",
    "It is possible to predict the annual salary for each customer using a predictive model. The hypothesis to be considered is that the annual salary for each customer can be estimated based on a couple of factors such as age and purchasing behaviour of the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6e19f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d2a67",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7e9b7",
   "metadata": {},
   "source": [
    "### 1.3 Project Goal <a id='pg'></a>\n",
    "\n",
    "In this project, we seek to achieve 2 main goals and they are;\n",
    "\n",
    "* Segment dataset and draw unique insights, including visualization of the transaction volume and assessing the effect of any outliers.\n",
    "* Explore correlation between customer attributes and build a regression and a decision-tree prediction model based on your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2502006",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a9202",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b8dea",
   "metadata": {},
   "source": [
    "### 1.4 Information Needed <a id='in'></a>\n",
    "\n",
    "In order to test the hypothesis of whether annual salary can be estimated using the age and purchasing behaviour of the customers, we would need to acquire the data needed to test the hypothesis and perform Exploratory Data Analysis. This will help us determine other factors that might help us predict annual salary of customers, consequently allowing us to make plausible decisions.\n",
    "\n",
    "We would need the following data to be able to perform EDA and build our model.\n",
    "1. **`Customer data`** - which should include characteristics of each customer, for example, age, education, transaction mode of customer etc.\n",
    "2. **`Salary data`** - which would indicate current salary of customers.\n",
    "3. **`Historical transaction data`** – which should indicate every transaction the customer has performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3fb97",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1d1ae",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d835c4f",
   "metadata": {},
   "source": [
    "### 1.5 Methodology <a id='my'></a>\n",
    "\n",
    "<p style=\"text-align:justify;\">The methodology that will be used for our project will largely depend on the goals we set out to achieve. The methodlogy framework below gives us a comprehensive guide on the methodology apparoach that will help us achieve our goals.</p>\n",
    "<br>\n",
    "<p style=\"text-align:center;font-weight:bold;font-size:20px\"> Methodology Framework</p>\n",
    "<br>\n",
    "<img src='https://artofdatablog.files.wordpress.com/2017/10/methodology-map.jpg' style=\"float:center;width:700px;\">\n",
    "\n",
    "Once we have the data, we would need to engineer features based on the data that we obtain, and build a model suitable for continuous numeric predictions (e.g., Linear Regression, Decision Tree, Random Forest, Gradient Boosted Machines to name a few), picking the most appropriate model based on the tradeoff between the complexity, the understanding, and the error margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb836bf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a58349",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb291380",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41566cd",
   "metadata": {},
   "source": [
    "## 2. Data Preparation <a id='dp'></a>\n",
    "\n",
    "[Move Up](#mu)\n",
    "\n",
    "An understanding of the data coupled with problem understanding will help us in cleaning and preparing our data for analysis. It is usually rare to acquire a ready-to-use data for any analysis without some level of preparation. To prepare our data, we normally assess the quality of the data, cleanse, format, blend and sample the data since we may encounter various issues with columns in our data. These issues may include:\n",
    "\n",
    "* **`Missing values:`** meaning column values are incomplete\n",
    "* **`Incorrect data:`** meaning you see values not expected for the column name\n",
    "* **`Inconsistent values:`** meaning some values may fall outside the expected range\n",
    "* **`Duplicate values:`** meaning whether or not there are duplicate values\n",
    "* **`Inconsistent data type:`** meaning values entered in the columns may not be consistent with the column names\n",
    "\n",
    "To properly prepare our data for analysis, we will perform two important tasks which are;\n",
    "\n",
    "* Part I: Data Quality Assessment\n",
    "* Part II: Data Cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c64e3d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087cd54",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7068c",
   "metadata": {},
   "source": [
    "### 2.1 Data Quality Assessment <a id='dqa'></a>\n",
    "\n",
    "<p style=\"text-align:justify;\">The first task that we will perform under the data preparation step is initial assessment of the quality of data which will easily allow us to properly clean our data. We will use this section to write any code necesary for inspecting the dataset. Once completed, we will leave our report in the Data Quality Report Document.\n",
    "\n",
    "At the end of our inspection, we will provide a summary of all of our findings.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48acd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from analyticViz.viz import *\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import scipy.stats as stats\n",
    "from scipy.special import inv_boxcox\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(color_codes=True)\n",
    "pd.set_option('display.max_columns',100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5271c71",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca955de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using pandas\n",
    "\n",
    "data = pd.read_csv('../data/anz.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd54a3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the shape of the dataframe\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b7c8b",
   "metadata": {},
   "source": [
    "> We can see from the above results that we have `12043` observations and `23` columns. The data is rich enough to help us perform our analysis as well as build the predictive models. However, we will have to assess the quality of the data and make the necessary cleaning before setting out to achieve our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567f691",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556096d3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4541b",
   "metadata": {},
   "source": [
    "In this step we will be assessing the quality of the data and make all the possible recommendation for cleaning this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb415a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a27d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6722c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e12d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ead51",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddda147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "\n",
    "data.duplicated(keep=False).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765a676",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7777aa",
   "metadata": {},
   "source": [
    "We will proceed by checking if the data contain any missing values. We can easily tell from the results of the info but we want to be extremely sure there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31064f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine the percentage of missing values in our data\n",
    "\n",
    "def missing_values(data):\n",
    "    \"\"\"Function that checks for null values and computes the percentage of null values\n",
    "    Args:\n",
    "        data: dataframe - data whose missing value is to be determined\n",
    "    Return:\n",
    "        missing_output: dataframe - dataframe of total null values with corresponding percentages\n",
    "    \"\"\"\n",
    "    total = data.isnull().sum().sort_values(ascending=False) \n",
    "    percentage = round((total / data.shape[0]) * 100, 2)\n",
    "    \n",
    "    missing_output = pd.concat([total, percentage], axis=1, keys=['Total','Percentage'])\n",
    "    \n",
    "    return missing_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the missing values function on data1\n",
    "\n",
    "miss_values = missing_values(data)\n",
    "miss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85736300",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the extent of the missing values\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(data.isnull(), cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290435a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bd211",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708e91e",
   "metadata": {},
   "source": [
    "#### Data Quality Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba1611",
   "metadata": {},
   "source": [
    "This data presents a lot of opportunity for data cleaning. This is because most of the features have issues that have to be resolved. Also, we have to determine columns will not be needed for various reasons and then consequently drop them.\n",
    "\n",
    "Base on the ouputs displayed above, here is the summary of the data quality issues which have to be fixed during the data cleaning process:<br>\n",
    "\n",
    "* Because of the huge null values in some of the columns, we will have to drop the entire column with missing values percentage greater than 60% in order to keep huge portion of our data. Columns to drop include:\n",
    "    * merchant_code\n",
    "    * bpay_biller_code\n",
    "\n",
    "\n",
    "* Also, we can see that there are some columns that will not be needed both for the analysis we seek to perform and model  building. These columns are:\n",
    "    * account (customer account numbers wont be needed for the task ahead)\n",
    "    * currency (because its all in AUD based on the summary statistics results - unique value = 1)\n",
    "    * country (because all customers are from Australia - unique value= 1)\n",
    "\n",
    "\n",
    "* We also need to properly format the data types for some of the columns which include:\n",
    "    * long_lat (split the values and format to int data type)\n",
    "    * date (format to datetime object)\n",
    "    * extraction (format to datetime objects. Similar to the date column and we might possibly drop it)\n",
    "    * merchant_long_lat (split and format to int data type)\n",
    "\n",
    "\n",
    "* Finally, based on the analysis we want to perform we might engineer new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb99ba",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf412d9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6a77d",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning and Preprocessing<a id='dcp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42486c63",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">The preprocessing step (usually an iterative process) is carried out to clean the data based on data quality issues identified. During the data quality assessment, we identified various data quality issues including missing values, incorrect data, inconsistent values, etc. \n",
    "\n",
    "In this task we will perform all the initial data cleaning and preprocessing needed to produce data that will be suitable for our analysis.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87f626",
   "metadata": {},
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "In this step, we will drop columns with high percentage of missing values. The columns with less percentage of missing values will be either dropped or imputed during model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89303d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dropping based on percentage of missing values (greater than 60%).\n",
    "\n",
    "data = data.drop(columns=['merchant_code', 'bpay_biller_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f2b21",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc181f49",
   "metadata": {},
   "source": [
    "#### Dropping Unneeded Columns\n",
    "\n",
    "We highlighted during the data quality assessment that there are some columns that won't be needed for both our analysis and model building. Therefore, we will have to drop those columns entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns have single values\n",
    "\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dropping irrelevant columns.\n",
    "\n",
    "data = data.drop(columns=['account', 'currency', 'country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1b9cb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba56baa",
   "metadata": {},
   "source": [
    "#### Date Formatting\n",
    "\n",
    "The date column for this data will be formatted to datetime. We must note that the **`extraction`** column contains both date and time that needs to be formatted to datetime. This column shows transactions performed for different times within a day but the date of the transaction is similar to the **`dates`** column. Since we only interested in the day the transaction was made and not the specific time, we will drop the extraction column and base our analysis on the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dropping extraction column.\n",
    "\n",
    "data = data.drop(columns=['extraction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17604495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dates to datetime objects for easy access of dates\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bac102",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2948b",
   "metadata": {},
   "source": [
    "#### Splitting Coordinates Column\n",
    "\n",
    "In this step we will split the values for both `long_lat` column and `merchant_long_lat` column and assign the values to both longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting and formatting long_lat column\n",
    "\n",
    "data['longitude'] = [float(data.split('-')[0].strip()) for data in data['long_lat']]\n",
    "data['latitude'] = [float(data.split('-')[1].strip()) for data in data['long_lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting and formatting merchant_long_lat column\n",
    "\n",
    "data['merchant_longitude'] = [float(data.split('-')[0].strip()) if str(data) != 'nan' else np.nan \n",
    "                                for data in data['merchant_long_lat']]\n",
    "    \n",
    "data['merchant_latitude'] =  [float(data.split('-')[1].strip()) if str(data) != 'nan' else np.nan \n",
    "                                for data in data['merchant_long_lat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f60dd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a244a7",
   "metadata": {},
   "source": [
    "After creating these new columns, we will have to remove the `long_lat` and `merchant_long_lat` columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping previous columns\n",
    "\n",
    "data = data.drop(columns=['long_lat', 'merchant_long_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a glimpse of the data\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00850f0d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c6fde",
   "metadata": {},
   "source": [
    "#### Data Type Formatting\n",
    "\n",
    "In this step we will format the columns to the correct data types which will make our analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b70439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data types\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4932f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all objects to category dtypes\n",
    "\n",
    "data[data.select_dtypes(['object']).columns] = data.select_dtypes(['object']).apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08761e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07d16e",
   "metadata": {},
   "source": [
    "After the data cleaning and preprocessing step comes the exploratory data analysis task. In this step we will explore our data to discover hidden trends and generate new insights which will lead us to better understand the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4900c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f4d5e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca436",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae54dc1",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis<a id='eda'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ddc97",
   "metadata": {},
   "source": [
    "One of the goals for this project as mentioned earlier is to segment dataset and draw unique insights, including visualization of the transaction volume and assessing the effect of any outliers. Based on this stated goal, we will perform any set of analysis to obtain insights that can help us arrive at some plausible conclusions.\n",
    "\n",
    "To achieve the first goal, we will look at general distirbutions of our features and try to answer the questions listed below:\n",
    "\n",
    "\n",
    "* Are males performing more transactions as compared to females?\n",
    "* What is the average spending by the customers?\n",
    "* Are most of the transactions authorized?\n",
    "* Between males and females, who spends a lot?\n",
    "* Which suburb do most of the transactions take place?\n",
    "* How does spending vary with state?\n",
    "* How did the average amount spent by customers changed over time ( days, weeks)?\n",
    "\n",
    "<b>NB: Questions that can be answered are not limited to the ones stated above.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd95772",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb7fcd",
   "metadata": {},
   "source": [
    "#### Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b97bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter(data, 'age', 'amount', 'Transaction Amount Vs Age', 'Age', 'Amount', color='gender', render='webgl', f_col='status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855ac2a",
   "metadata": {},
   "source": [
    "> For authorized transactions, we can see that huge part of the amounts transacted are relatively low and this is mostly spread among customers with ages between `20` and `50` years. Also, the highest authorized transaction amount was completed by a female customer despite the fact that we have more males performing high amount transactions. On the flip side, posted transaction amounts are approximately well spread out amount males and females but still dominated by younger to adult customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fa5ec",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data, 'age', 'balance', 'Account Balance Vs Age', 'Age', 'Balance', color='gender', render='webgl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5b990",
   "metadata": {},
   "source": [
    "> Customers with high account balances are mostly aged between 35 to 50 years. Most customers have account balances ranging between` 1` to `25000 AUD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7016e84",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box(data, 'gender', 'balance', 'Account Balance Per Gender', 'Gender', 'Balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8b7f6",
   "metadata": {},
   "source": [
    "> On average male customers tend to have a lot of money in their accounts as compared to females, that is `7967 AUD` to `4883 AUD`. However, this is not conclusive as the difference might be due to a lot of factors like outliers which are clearly visible in the box plot or other extraneous factors. We will there need to confidently conclude the difference using statistical analysis. \n",
    "\n",
    "> Looking at the graph, we can also see that `75%` of female customers have at most approximately `8990 AUD` available in their accounts whereas `75%` of male customers have at most approximately `15658 AUD` in their accounts.\n",
    "\n",
    "> That said, we can see there are a lot of outliers in the dataset which we will need to handle later during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7d83f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03444031",
   "metadata": {},
   "outputs": [],
   "source": [
    "box(data, 'gender', 'amount', 'Transaction Amount Per Gender', 'Gender', 'Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3deea",
   "metadata": {},
   "source": [
    "> Looking at the transaction amount per gender, we can see that the distribution looks relatively the same despite the fact that males transact approximately `2 AUD` on average than females. Again, we can see some outliers in the distribution which we will look at.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c42a2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data, 'age', 'Age Distribution of Customers', 'box', color='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab13214",
   "metadata": {},
   "source": [
    "> The age distribution of customers is slightly skewed to the right due to the few outliers we have in the dataset. The age ranges from `18 - 78 years` for males and `18 - 64 years` for females. The average age is approximately close at `28` for male and `27` for females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188664c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data, 'amount', 'Amount Distribution of Customers', 'box', color='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d727fc",
   "metadata": {},
   "source": [
    "> The distribution is right skewed with only a few customers performing huge amount transactions. In fact `75%` of the customers transacted `55 AUD` or less. The maximum amount transacted by males is approximately `9000 AUD` and approximately `7000 AUD` for females. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b69d6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5fbcc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0946a7d",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631101c8",
   "metadata": {},
   "source": [
    "#### Question 1: Are males performing more transaction amounts as compared to females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4777a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sun(data, ['gender', 'movement', 'txn_description'], 'amount', 'Transaction Amount Per Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4816e9c",
   "metadata": {},
   "source": [
    "> The total amount transacted by males stands at `1,292,961 AUD` and that of females stands at `970,322 AUD`. There is a huge difference of about `300,000 AUD` which again might be due to more male customers as compared to females. In addition, huge part of the total amount was `credit` transactions which were mainly `salary` payments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f22f1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa663a",
   "metadata": {},
   "source": [
    "#### Question 2: What is the average spending by the customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c6964",
   "metadata": {},
   "source": [
    "For this question we will be using the `movement` and `amount` column. That said, we must note that we only talking about customer average spending which means, we looking at money moving out of their accounts. Therefore, we will only select transactions that resulted in a debit and then compute average amount for those transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmenting data for only debit transactions\n",
    "\n",
    "debit_data = data[data['movement'] == 'debit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average spend\n",
    "\n",
    "deb_dat = debit_data[['customer_id', 'amount', 'first_name', 'gender']]\n",
    "dat = deb_dat.groupby(['customer_id', 'first_name', 'gender']).mean().sort_values(by='amount', ascending=False)\n",
    "dat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbar(dat.head(20), 'amount', 'customer_id', 'amount', 'Top 20 Spenders', 'AVG Spend', 'Customer', color='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19016bf",
   "metadata": {},
   "source": [
    "> The top spenders are mostly males with only 5 females appearing in the list. Top male spender spends about `190 AUD` on average whereas top female spender spends `160 AUD` on average as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316bf54",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6de75b",
   "metadata": {},
   "source": [
    "#### Question 3: Are most of these transactions authorized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3d249",
   "metadata": {},
   "source": [
    "We will use the `status` column to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_stat = data[['customer_id', 'status', 'gender']]\n",
    "status = tran_stat.groupby(['status', 'gender']).count()\n",
    "status.reset_index(inplace=True)\n",
    "status.rename(columns={'customer_id':'count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vbar(status, 'status', 'count', 'count', 'Transactions Authorized per Gender', 'Status', 'Count','gender', 'stack')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22886e",
   "metadata": {},
   "source": [
    ">`Authorized` transactions are transactions that have been approved and account yet to be debited whereas `posted` transactions have already been processed and accounts have been debited. In relation to our analysis, we can see that most customers have performed transactions and are yet to be debited. However, completed transactions are about `5000` approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f31c8b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c53641",
   "metadata": {},
   "source": [
    "#### Question 4: Which suburb do most of the transactions take place?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b7883",
   "metadata": {},
   "source": [
    "We now want to look at debit transactions performed in each suburb. This will give us a clue as to which suburb do most customers spend their money. We will be using the `merchant_suburb` column from the debit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(str(review) for review in debit_data['merchant_suburb'].dropna())\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 5), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Suburb with Highest Number of Debit Transactions\\n\", size=15)\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05084d38",
   "metadata": {},
   "source": [
    "> It is not surprising that `Sydney` and `Melbourne` stand out as the suburbs that customers prefer to spend their money. An online [blog post](https://www.wsfm.com.au/entertainment/these-sydney-suburbs-are-spending-the-most-when-it-comes-to-online-shopping) highlighted the spending habits in these two suburbs. We can also see other surburbs like `Brisbane City`,`Southport`,`Adelaide` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564f2f6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366138c",
   "metadata": {},
   "source": [
    "#### Question 5: How does spending vary with state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673ecb3",
   "metadata": {},
   "source": [
    "We will visualize the spending amount for each state. To do this we will use the `merchant_state` and `amount` columns of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassigning the names of the states\n",
    "\n",
    "new_state = {'QLD': 'Queensland', 'NSW': 'New South Wales', 'VIC': 'Victoria', 'WA': 'Western Australia', \n",
    "             'SA': 'South Australia', 'NT': 'Northern Territory', 'TAS': 'Tasmania', 'ACT': 'Australian Capital Territory'}\n",
    "\n",
    "\n",
    "map_data = data[data['movement'] == 'debit']\n",
    "map_data = map_data.replace({'merchant_state': new_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987803f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map = map_data.loc[:, ['merchant_state', 'amount']]\n",
    "now = plot_map.groupby('merchant_state').sum()\n",
    "now.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file containing properties for each state\n",
    "state = json.load(open('../australian-states.min.geojson', 'r'))\n",
    "\n",
    "\n",
    "# creating a mapping between id and states\n",
    "state_id_map = {}\n",
    "for feature in state['features']:\n",
    "    state_id_map[feature['properties']['STATE_NAME']] = feature['id']\n",
    "    \n",
    "    \n",
    "# assigning unique id to states in data \n",
    "now['id'] = now['merchant_state'].apply(lambda x: state_id_map[x])\n",
    "now['id'] = now['id'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the map of debit transactions for each state\n",
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "    now,\n",
    "    locations=\"id\",\n",
    "    geojson=state,\n",
    "    color=\"amount\",\n",
    "    hover_name=\"merchant_state\",\n",
    "    hover_data=[\"amount\"],\n",
    "    title=\"Spending Per State\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    center={\"lat\": -26, \"lon\": 133},\n",
    "    color_continuous_scale=px.colors.diverging.BrBG,\n",
    "    color_continuous_midpoint=0,\n",
    "    zoom=3,\n",
    "    opacity=0.7,\n",
    ")\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25357df",
   "metadata": {},
   "source": [
    "> We can see from the visualization above that `New South Wales` have the highest amount spent with a total amount of about `102,000 AUD` and they are closely followed by `Victoria` with an amount of approximately `88,000 AUD`. `Queensland`, `Western Australia`, `South Australia`, `Northern Territory` and `Tasmania` also follow in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae6e62",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffba6ae",
   "metadata": {},
   "source": [
    "#### Question 6: How did the amount spent by customers change over time ( days, weeks)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d9c67",
   "metadata": {},
   "source": [
    "Now we want to analyse the spending habits of customers over time. In this task we will look at spending habits per gender over time. We will need the date and amount columns to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = debit_data.loc[:, ['date', 'amount', 'gender']]\n",
    "amount_per_day = pd.DataFrame({'median' : dat.set_index('date').groupby('gender'). \\\n",
    "                               resample('D')[\"amount\"].median()}).reset_index()\n",
    "amount_per_week = pd.DataFrame({'median' : dat.set_index('date').groupby('gender'). \\\n",
    "                               resample('W')[\"amount\"].median()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be570a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "line(amount_per_day, 'date', 'median', 'Time', 'Amount Spent', 'Daily Average Spend Per Gender', color='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "line(amount_per_week, 'date', 'median', 'Time', 'Amount Spent', 'Weekly Average Spend Per Gender', color='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c1195",
   "metadata": {},
   "source": [
    ">There is no defined trend for daily average spending amount from the above visualization. However, we can see the highest average spend in a day was on `Sept 17, 2018` with an amount of `35.5 AUD` and the lowest average spend was recorded on `Oct 30, 2018` with an amount of `20 AUD`. We expect to see rise in spend during the festive periods and its a bit surprising that the average spend has not started peaking already instead we only see an average spend of about `25 AUD` as at `24 November, 2018`. Also, there is no record for `August 16, 2018` which is a situation that will need further investigation. We must note that the distribution of the amount is skewed as we saw from the second question. Therefore, in order to properly analyze the average amount spend we opted to go with the median.\n",
    "For the weekly average spend, we can see a trend here. We realise that the average spend decreases for the last week of every month. It could be that most customer might have end up spending large portion of their salary and are now managing what's left thus the decrease in average spend in the final week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde3da5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf56a3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc877da",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ca92f",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis<a id='sa'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b9ec2",
   "metadata": {},
   "source": [
    "For this task we will further look into the question that was asked about the spending habit of customers based on their gender. We found out that the number of male customers performed a lot of debit transactions than their female counterparts. We will calculate the average amount spent by both genders and then conclude on which gender spends more using statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend = debit_data[['amount', 'gender']]\n",
    "spend_habit = spend.groupby(['gender']).mean()\n",
    "spend_habit.reset_index(inplace=True)\n",
    "spend_habit.rename(columns={'amount':'average_spend'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_habit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d71c22",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476696d",
   "metadata": {},
   "source": [
    ">From the above dataframe, we can see that the average spend for males is approximately `5 AUD` more than that of females. From this result we can easily conclude that males spend a lot as compared to females. However, what if the result could be due to more males than females, or vise versa but with high amount spent. We will therefore need to clearly conclude without any doubt that males spend more than females and that the difference is not due to chance. To do this we will perform hypothesis testing to draw conclusion  on the result.\n",
    "In order to select a particular test to use, we will look at the distribution of the `amount` column. If the distribution is normal then we will go ahead and use the `Welch's t-Test` since there's unequal number of males and females. However, if the distribution is not normal, then we will transform the data and later apply the Welch's t-Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f708320",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2706e2",
   "metadata": {},
   "source": [
    "We will go ahead and look at the sample distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(spend, 'amount', 'Distribution of Customer Spend', 'box', 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809ff00",
   "metadata": {},
   "source": [
    ">From the distribution above we can see that our data is skewed to the right which may be due to a lot of outliers in our data (rare spending value of customers). These outliers may affect the overall spending value of the customers and might lead to bias conclusions. In order to curb the impact of these outliers on our results, we will go ahead and remove them and then compute the averages again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6208ae",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aace20",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84553784",
   "metadata": {},
   "source": [
    "### 4.1 Removing Outliers<a id='ro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c4960",
   "metadata": {},
   "source": [
    "Lets go ahead and detect the outliers in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect outliers\n",
    "\n",
    "def remove_outlier(data, column):\n",
    "    \"\"\"\n",
    "    Function that removes outliers from the dataframe\n",
    "\n",
    "    Args:\n",
    "        data : pandas dataframe\n",
    "            Contains the data where the outliers are to be found\n",
    "        column : str\n",
    "            Usually a string with the name of the column\n",
    "    \n",
    "    Returns:\n",
    "        None: prints number of outliers and then removes all the outliers\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate interquartile range\n",
    "    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "    iqr = q75 - q25\n",
    "    print('Percentiles: 25th = %.3f, 75th = %.3f, IQR = %.3f' % (q25, q75, iqr))\n",
    "    \n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    \n",
    "    # identify outliers\n",
    "    indx = np.where((data[column] < lower) | (data[column] > upper))\n",
    "    print('Identified outliers: %d' % len(indx[0]))\n",
    "    \n",
    "    # remove outliers\n",
    "    data.drop(data.iloc[indx].index, inplace=True)\n",
    "    print('Non-outlier observations: %d' % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outlier(spend, 'amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1f7e9",
   "metadata": {},
   "source": [
    "> From the results, we were able to detect and remove `1182` outliers in total using the interquartile range approach. We will now go ahead and look at the average spend and the resulting distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8542f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d81f1d",
   "metadata": {},
   "source": [
    "Visualize after removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(spend, 'amount', 'Distribution of Customer Spend', 'box', 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf91cb",
   "metadata": {},
   "source": [
    "> From the graph above, we can see that the distribution for each gender is still skewed to the right. We will perform the normality test and perform all the needed transforms to make our distribution normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b0381",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12cd11",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420c8a9",
   "metadata": {},
   "source": [
    "### 4.2 Normality Test<a id='nt'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d2673",
   "metadata": {},
   "source": [
    "The next step after removing outliers is to confirm the normality of the distribution to ensure we select the right statistical technique. We will quantify whether the distribution is normal or not by using the `Shapiro-Wilk test`.\n",
    "\n",
    "**Null hypothesis:** <br>\n",
    "<br>\n",
    "$H_0:$ `Distribution is normal`\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Alternative hypothesis:** <br>\n",
    "<br>\n",
    "$H_1:$ `Distribution is not normal`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**`Interpretation of Normality Test`**\n",
    "\n",
    "- p-value $\\leq$ alpha: significant result, reject null hypothesis, not Gaussian (H1).\n",
    "- p-value $>$ alpha: not significant result, fail to reject null hypothesis, Gaussian (H0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normality test\n",
    "\n",
    "stat, p = stats.shapiro(spend['amount'])\n",
    "print('Statistics={:.3f}, p={:.3f}'.format(stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d5704",
   "metadata": {},
   "source": [
    ">The distribution is not normal which is expected because of the skewed distribution we saw earlier. In order to perform parametric hypothesis test we will need to transform the distribution to a normal distribution and in this case we will be using the `box-cox` transform. But we will go ahead and test for homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f6c9c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e98497",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c8d2e",
   "metadata": {},
   "source": [
    "### 4.3 Homogeneity of variances<a id='hov'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce097afe",
   "metadata": {},
   "source": [
    "The next assumption to test before performing hypothesis testing is homoscedasticity or Homogeneity of variances. That is, determine if the variances are equal between treatment groups. We can use `Levene’s`, `Bartlett’s`, or `Brown-Forsythe test` based on where sample was taken from. If sample was drawn from a normal distribution then `Bartlett's test` can be used otherwise use either `Levene's` or `Brown-Forsythe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7392928",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.levene(spend[spend['gender']=='M']['amount'], spend[spend['gender']=='F']['amount'])\n",
    "print('Statistics={:.3f}, p={:.3f}'.format(stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Equal Variances (fail to reject H0)')\n",
    "else:\n",
    "    print('Unequal Variances (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d3ad3",
   "metadata": {},
   "source": [
    "> We can also see that there exists unequal variances in the samples for both male and female spend. Now we will go ahead and transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa774987",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995a0db",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf27ff",
   "metadata": {},
   "source": [
    "### 4.4 Box-Cox Transformation<a id='bct'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd657ac",
   "metadata": {},
   "source": [
    "If after performing normality test and we find out our data is not normally distributed, then we will go ahead and use any suitable transformations. One example is `Box-Cox Transaformation`.\n",
    "\n",
    "The Box-Cox method is a data transform method that is able to perform a range of power transforms, including the log and the square root. It can be thought of as a power tool to iron out power-based change in your data sample. The resulting data sample may be more linear and will better represent the underlying non-power distribution, including Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend['amount'], fit_lambda = stats.boxcox(spend['amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa3376",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9b401",
   "metadata": {},
   "source": [
    "Now we will go ahead and visualize our data to check the impact of the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e57e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(spend, 'amount', 'Distribution of Customer Spend', 'box', 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da62dd8",
   "metadata": {},
   "source": [
    "> Now we can see that our distribution is roughly normal which satisfies the condition for performing Welch's t-Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d024fc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ba5b2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e148092",
   "metadata": {},
   "source": [
    "### 4.5 Hypothesis Test <a id='ht'></a>\n",
    "\n",
    "In statistics, a `hypothesis test` is used to assess and understand the plausibility, or likelihood of some hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c4142",
   "metadata": {},
   "source": [
    "#### 4.5.1 t-Test\n",
    "\n",
    "`Welch's t-test`, or unequal variances t-test, is a two-sample location test which is used to test the hypothesis that two populations have equal means. It is more reliable when the two samples have unequal variances and/or unequal sample sizes. In our case the number of male counts is greater than female counts which makes this test the best choice for testing our hypothesis.\n",
    "\n",
    "<br>\n",
    "\n",
    "**`Welch's t-Test Setup`**\n",
    "\n",
    "**Null hypothesis:** <br>\n",
    "<br>\n",
    "$H_0:$ $μ_1$ = $μ_2$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Alternative hypothesis:** <br>\n",
    "<br>\n",
    "$H_1:$ $μ_1$ $\\neq$ $μ_2$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**`Interpretation of Welch's T-Test`**\n",
    "\n",
    "- p-value $\\leq$ alpha: `Different distributions (reject H0)`\n",
    "- p-value $>$ alpha: `Same distributions (fail to reject H0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b12ac",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8ad4f",
   "metadata": {},
   "source": [
    "Now we will go ahead and perform the test. The only difference between the `welch's t-test` and `student t-test` is basically the difference in variance. The latter assumes the variance to be the same(homogeneity of variance) whereas the former doesn't. Therefore, we will use the scipy module `ttest_ind` but we will specify `equal_var` to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb149769",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.ttest_ind(spend[spend['gender']=='M']['amount'], spend[spend['gender']=='F']['amount'], equal_var=False)\n",
    "print('Statistics={:.3f}, p={:.3f}'.format(stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fc657",
   "metadata": {},
   "source": [
    "> Based on the outcome of the hypothesis test we can conclude there is a significant difference between the average spend for male and female customers. And this buttresses our initial assumption that male spend more than females base on our data set. To properly use this result to estimate the entire population we will go ahead and compute the confidence interval for average spend for both male and female customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa695242",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336413e4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807816c4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a19298",
   "metadata": {},
   "source": [
    "### 4.6 Effect Size<a id='es'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba7ab2",
   "metadata": {},
   "source": [
    "`Cohen’s D` , or standardized mean difference, is one of the most common ways to measure `effect size`. An `effect size` is how large an effect is. For example, medication A has a larger effect than medication B. While a p-value can tell you if there is an effect, it won’t tell you how large that effect is.\n",
    "\n",
    "Cohen’s D specifically measures the effect size of the difference between two means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohend(d1, d2):\n",
    "    # calculate the size of samples\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    \n",
    "    # calculate the variance of the samples\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    \n",
    "    # calculate the pooled standard deviation\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # calculate the means of the samples\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    \n",
    "    # calculate the effect size\n",
    "    return (u1 - u2) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cohend(spend[spend['gender']=='M']['amount'], spend[spend['gender']=='F']['amount'])\n",
    "print('Cohens d: %.3f' % d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d71bf",
   "metadata": {},
   "source": [
    "> Per the hypothesis test that we performed, we concluded that there is significant difference between male and female spend. To properly quantify this, the `cohen's d` value indicates that the two sample means differ by approximately 1 standard deviation. Despite the fact that the effect is small, the difference is still significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c389c4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13798f96",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7dd98",
   "metadata": {},
   "source": [
    "### 4.7 Confidence Interval<a id='ci'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb3a42",
   "metadata": {},
   "source": [
    "Now that we know and can confidently say that there is significant difference in average spend, how does this reflect in the entire population. How can we estimate the average spend for either customer at any point in time? This is where we need confidence interval to give us a range in which the average spend for each customer in a population will fall.\n",
    "\n",
    "We will go ahead and compute the confidence interval for both male and female average spend. Now we must know that the values we going to get will be the transformed values. To get the average spend values, we will have to use `inverse box-cox` transformation on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5384da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend['amount'] = inv_boxcox(spend['amount'], fit_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = spend[spend['gender']=='M']['amount']\n",
    "x2 = spend[spend['gender']=='F']['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the confidence interval\n",
    "print(\"\"\"-------------------------------------------------------------\n",
    "Confidence Interval - Average Spend Range for Male Customers\n",
    "-------------------------------------------------------------\\n\"\"\")\n",
    "print('Confidence Interval: {}\\n\\n'.format(stats.t.interval(alpha=0.95, df=len(x1)-1, loc=np.mean(x1),scale=stats.sem(x1))))\n",
    "\n",
    "\n",
    "print(\"\"\"-------------------------------------------------------------\n",
    "Confidence Interval - Average Spend Range for Male Customers\n",
    "-------------------------------------------------------------\\n\"\"\")\n",
    "print('Confidence Interval: {}'.format(stats.t.interval(alpha=0.95, df=len(x2)-1, loc=np.mean(x2), scale=stats.sem(x2)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ce978",
   "metadata": {},
   "source": [
    ">From the confidence interval, if we repeated this over and over again, 95 percent of the time, the average spend for male would fall somewhere between.`28.2 AUD` and `29.2 AUD` and average female spend would fall between `26.4 AUD` and `27.4 AUD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e69282",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0291d85",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd21ef5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77caef",
   "metadata": {},
   "source": [
    "## 5. Modeling<a id='dm'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18dca6c",
   "metadata": {},
   "source": [
    "To remind ourselves, we stated earlier as one of the objectives to build a model that is able to predict annual salary of customers. Based on the methodology framework we saw at the beginning, we know that our problem is a continuous numeric prediction problem. We also highlighted that, we will have to build multiple models and then finally select the best one based on certain evaluation metrics. \n",
    "\n",
    "Therefore, to complete this task we will go through the various machine learning steps which includes;\n",
    "\n",
    "* Data Understanding\n",
    "* Feature Engineering\n",
    "* Splitting Dataset\n",
    "* Algorithm Evaluation\n",
    "* Parameter Tuning\n",
    "* Final Model\n",
    "* Model Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee361a61",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6a01c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c30ba3",
   "metadata": {},
   "source": [
    "### 5.1 Data Understanding<a id='du'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7976e70",
   "metadata": {},
   "source": [
    "In this section, we will probe the data to understand how the features in the data relate. This will inform us on which new variables to engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset\n",
    "\n",
    "ml_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b29d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded columns\n",
    "\n",
    "ml_data.drop(columns=['merchant_id', 'first_name', 'transaction_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db853755",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2520dc1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3f189",
   "metadata": {},
   "source": [
    "#### 5.1.1 Descriptive Statistics<a id='ds'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd09baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types and counts of our data\n",
    "\n",
    "ml_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attain summary statistics - add include='all' to see that for categorical data\n",
    "\n",
    "ml_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of the data\n",
    "\n",
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the correlation coefficient\n",
    "\n",
    "pd.set_option('precision', 5)\n",
    "ml_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933b589",
   "metadata": {},
   "source": [
    "> We can see that there are different ranges for numerical values with different standard deviations which will clearly hamper the performance of our ml model. To resolve this issue we will have to `standardize` the dataset and ensure that all numeric values revolve around the same `mean and std`. Also, we observe that some of the columns have `missing values` and some are also `strongly correlated`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351c65a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039ee5b",
   "metadata": {},
   "source": [
    "#### 5.1.2 Data Visualization<a id='dv'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(ml_data, \n",
    "                        dimensions=['balance', 'age', 'amount','card_present_flag', 'movement'],\n",
    "                        color='gender',\n",
    "                        color_discrete_sequence=px.colors.qualitative.D3)\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684a880",
   "metadata": {},
   "source": [
    "> We can see there are a few columns with binary values and the scatter plots show good relationship between some of the columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1ed16",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b352029",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fe82b",
   "metadata": {},
   "source": [
    "### 5.2 Feature Engineering and Selection<a id='fes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e1377",
   "metadata": {},
   "source": [
    "The better we prapare our data for the machine learning model, the better our prediction will be. In this task, we will properly prepare our data by transforming columns, dropping irrelevant columns, handling missing and categorical values and finally merging if the need be.\n",
    "\n",
    "We will first determine the best way to compute annual salary for the customers. We must understand that the transaction dataset contains different types of transactions completed by each customer. Therefore in order to compute the salary for each customer we will have to filter our dataset to contain only salary transactions. To do this we will use the`txn_description` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5168202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer salary\n",
    "\n",
    "salary = ml_data[ml_data['txn_description'] == 'PAY/SALARY']\n",
    "salary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14dd81",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ca1c0",
   "metadata": {},
   "source": [
    ">Looking at the unique values we can clearly see that the transactions only pertain to salary transactions. That is, because its a salary transaction, we expect the status to be only `posted`. Also, the movement should be `credit` since customer's account will be credited with their salaries. No merchant details because this is not a spending activity and as expected the customer ID is also `100`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0858c0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e35cbd",
   "metadata": {},
   "source": [
    "Because the accounts of the customers were credited with their salaries, we will assume that customers were credited with the same amount of money at different intervals in the month for different customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data[(ml_data['customer_id'] == 'CUS-1462656821') & (ml_data['txn_description'] == 'PAY/SALARY')].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317f3a6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86470b",
   "metadata": {},
   "source": [
    "> From the dataframes above, we can observe that different customers receive salaries at different time intervals. Some receive for every week and others every 2 weeks. We will therefore go ahead and assume that at the end of every month, they receieve the same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04cceb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e9c2a",
   "metadata": {},
   "source": [
    "Now the next step will be to calculate the annual salary for each customer and also engineer new features based on the `amount`, `balance`, and `date` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all salary transactions\n",
    "\n",
    "salary_trans = ml_data[ml_data['txn_description'] == 'PAY/SALARY']\n",
    "salary_freq = salary_trans[['customer_id', 'date', 'amount', 'balance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get month names since year remains constant\n",
    "\n",
    "month_data = salary_freq[['customer_id', 'date', 'amount']]\n",
    "month_data['month'] = month_data['date'].dt.month_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6870a1e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5df01",
   "metadata": {},
   "source": [
    "We will now compute the number of times customers received salary in a particular month. Based on the inspection above, we noticed that customers received same salary at different frequencies for each month. Some had salaries every 2 weeks and others received it every week.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the frequency of salary payments in a month\n",
    "\n",
    "payment_freq = pd.DataFrame(month_data.groupby(['customer_id', 'amount'])['month'].value_counts())\n",
    "payment_freq.rename(columns={'month': 'monthly_freq'}, inplace=True)\n",
    "payment_freq.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# calculate salary for each month based on frequency\n",
    "\n",
    "payment_freq['monthly_sal'] = payment_freq['amount'] * payment_freq['monthly_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc089b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58ba61",
   "metadata": {},
   "source": [
    "We were able to compute `monthly salary` for each customer based on their respective frequencies. Now we will go ahead and compute their `average monthly salary` and `average monthly frequency` for the unqiue number of months they received their salaries. After that, we will then compute their annual salary by multiplying the average monthly salary by 12 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average monthly salary based on frequency\n",
    "\n",
    "monthly_salary = payment_freq.groupby('customer_id')['monthly_freq', 'monthly_sal', 'amount'].mean().reset_index()\n",
    "monthly_salary.rename(columns={'monthly_freq':'avg_monthly_freq', 'monthly_sal':'avg_monthly_sal', \n",
    "                               'amount':'avg_trans_amt'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd46ab",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03717b8d",
   "metadata": {},
   "source": [
    "Now we will engineer new features based on the columns we have and finally merge all the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad12f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number of months salary was received \n",
    "\n",
    "number_of_months = payment_freq.groupby('customer_id')['month'].nunique().reset_index()\n",
    "monthly_sum = payment_freq.groupby('customer_id')['monthly_freq', 'monthly_sal'].sum().reset_index()\n",
    "cust_bal = salary_freq.groupby('customer_id')['balance'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ececd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes \n",
    "\n",
    "monthly_data = pd.merge(monthly_salary, number_of_months, on='customer_id')\n",
    "bal_data = pd.merge(monthly_data, cust_bal, on='customer_id')\n",
    "final_output = pd.merge(bal_data, monthly_sum, on='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56422864",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aa449",
   "metadata": {},
   "source": [
    "The final step is to calculate the annual salary usng the average monthly salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89215b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['annual_salary'] = final_output['avg_monthly_sal'] * 12\n",
    "final_output.rename(columns={'balance':'avg_balance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae48c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961d507",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcf61a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f986afe",
   "metadata": {},
   "source": [
    "Another feature we can engineer is to look at the spending habit of customers. We already confirmed that male spend more than females based on this dataset. Therefore, having the spending ratio can also help us in predicting customer salaries based on the assumption that `high spending customers earn high salary`. The spending ratio is to basically tell us how much of customers annual income do they spend. We will use the debit data we extracted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3108f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating average amount spent by each customer\n",
    "\n",
    "spend = debit_data.pivot_table(index='customer_id',values='amount', aggfunc=np.sum)\n",
    "spend['annual_spend'] = round(spend['amount'] * 4, 2)\n",
    "spend.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the created dataframes\n",
    "\n",
    "final_output = pd.merge(final_output, spend.drop(columns=['amount']), on='customer_id')\n",
    "final_output['spending_ratio'] = round(final_output['annual_spend'] / final_output['annual_salary'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain summary statistics\n",
    "\n",
    "final_output.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c81e6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8d9ce",
   "metadata": {},
   "source": [
    "> We can see that the highest customer percentage spend is `85%` and the minimum is `6%`. Also, 50% of customers have a percentage of `33%` which means `50%` of customers save or invest approximately `70%` of their income. Also, average number of times customers receive salary is at a maximum of `5`and minimum of `1`. The highest average monthly salary is `11781 AUD` whereas the minimum is `2385 AUD`. What we can infer from this statistics is that most customers spend less than `50%` of their income so investment solutions or products can be sold out to customers ensuring that the bank makes profit on the income saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25965717",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f643b7a",
   "metadata": {},
   "source": [
    "Next task is to append the `age` and `gender` of each customer to the final output before moving on to building our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting age and gender columns\n",
    "\n",
    "other_info = ml_data[['customer_id', 'age', 'gender']]\n",
    "other_info.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ec9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to bin age values\n",
    "\n",
    "def age_bins(data):\n",
    "    \"\"\"Function that groups age values\n",
    "    \n",
    "    Args:\n",
    "        data: dataframe - data that contains age column\n",
    "    \n",
    "    Returns:\n",
    "        groups: str - various groupings (young, adult, old)\n",
    "    \"\"\"\n",
    "    if data['age'] <= 20:\n",
    "        return 'young'\n",
    "    elif data['age'] > 20 and data['age'] <= 40:\n",
    "        return 'adult'\n",
    "    else:\n",
    "        return 'old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function and merge output data\n",
    "\n",
    "other_info['age_bin'] = other_info.apply(age_bins, axis=1)\n",
    "final_output = pd.merge(final_output, other_info, on='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331aa72",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cbb33",
   "metadata": {},
   "source": [
    "We will also go ahead and bin the speding ratio values into high, medium and low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to bin speding ratio values\n",
    "\n",
    "def spend_ratio_bins(data):\n",
    "    \"\"\"Function that groups spending ratios\n",
    "    \n",
    "    Args:\n",
    "        data: dataframe - data that contains spending ratio column\n",
    "    \n",
    "    Returns:\n",
    "        groups: str - various groupings (low, medium, high)\n",
    "    \"\"\"\n",
    "    if data['spending_ratio'] <= 0.3:\n",
    "        return 'low'\n",
    "    elif data['spending_ratio'] > 0.3 and data['spending_ratio'] <= 0.6:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b381a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['ratio_bins'] = final_output.apply(spend_ratio_bins, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.drop(columns='customer_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbda8b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e4823",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918c5e0",
   "metadata": {},
   "source": [
    "The final task will be to properly format the data types of each column before proceeding to training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['age_bin'] = final_output['age_bin'].astype('category')\n",
    "final_output['ratio_bins'] = final_output['ratio_bins'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for later\n",
    "\n",
    "final_output.to_csv('ml_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff65065",
   "metadata": {},
   "source": [
    "> Now our data is ready for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c91605",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d952610",
   "metadata": {},
   "source": [
    "### 5.3 Splitting Dataset<a id='sd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acd940",
   "metadata": {},
   "source": [
    "It is a good idea to use a test hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to evaluate the performance of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on models performance on unseen data. We will use 80% of the dataset for modeling and hold back 20% for validation.\n",
    "\n",
    "We will begin by importing the needed libraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b4246",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d31d6",
   "metadata": {},
   "source": [
    "Now we will go ahead and split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = final_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the correlation coefficient\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f48940",
   "metadata": {},
   "source": [
    "> From the output dataframe, we can see that `avg_monthly_sal` is highly correlated with both `monthly_sal` and `annual_salary` and its the same for `monthly_sal`. Therefore, to prevent multicolinearity, we will drop these two columns and work with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93810f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop highly correlated columns\n",
    "\n",
    "train_data = train_data.drop(columns=['avg_monthly_sal', 'monthly_sal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737364a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "\n",
    "X = train_data.drop('annual_salary', axis=1)\n",
    "y = train_data['annual_salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05ce68",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e4bd7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63cd4b",
   "metadata": {},
   "source": [
    "### 5.4 Algorithm Evaluation<a id='ae'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e1412",
   "metadata": {},
   "source": [
    "We don't know which algorithms will do well on this dataset. We will use 10-fold cross validation to evaluate the algorithms.\n",
    "\n",
    "We will evaluate algorithms using the appropriate metric. Firstly, we will begin by evaluating linear algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1b183",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289966d2",
   "metadata": {},
   "source": [
    "#### 5.4.1 Linear Algorithm Evaluation<a id='lae'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93524b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5559e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1680914",
   "metadata": {},
   "source": [
    "Now the next step is to define our pipeline. That is the series of steps we want to be performed before model is trained and saved. In this section we will perform median imputation for numeric missing values and then standardize numeric values. For the categorical values, we will simply perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define pipelines for various columns in our dataset\n",
    "# numeric columns will be standardize and missing values replaced with median\n",
    "\n",
    "numeric_features = list(X.drop(columns=['age_bin', 'gender', 'ratio_bins']).columns)\n",
    "numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "# categorical values will be encoded\n",
    "categorical_features = ['age_bin', 'gender', 'ratio_bins']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "# put everything together in a column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", numeric_transformer, numeric_features),\n",
    "                                               (\"cat\", categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d5766",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6c276",
   "metadata": {},
   "source": [
    "Next step is to define our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd7e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple algorithm evaluation\n",
    "\n",
    "pipes = []\n",
    "pipes.append(('LinearRegression', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())])))\n",
    "pipes.append(('Lasso', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", Lasso())])))\n",
    "pipes.append(('Ridge', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", Ridge())])))\n",
    "pipes.append(('LassoLars', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", LassoLars())])))\n",
    "pipes.append(('ElasticNet', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", ElasticNet())])))\n",
    "pipes.append(('Lars', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", Lars())])))\n",
    "pipes.append(('BayesianRidge', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", BayesianRidge())])))\n",
    "pipes.append(('ARDRegression', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", ARDRegression())])))\n",
    "pipes.append(('PAR', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", PassiveAggressiveRegressor())])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92112",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2ab70",
   "metadata": {},
   "source": [
    "Now we will go ahead and perform cross validation to evaluate the performance of selected algorithms on different sections of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(folds, scoring, pipeline = pipes):\n",
    "    \"\"\"Function for cross validation using piepelines\n",
    "    Args:\n",
    "        pipes - list: list of pipelines to be evaluated\n",
    "    Returns:\n",
    "        fig - dataframe: dataframe showing output results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    names = []\n",
    "    data_list = []\n",
    "    \n",
    "    for name, pipeline in pipes:\n",
    "        cv_results = cross_val_score(pipeline, X_train, y_train, cv=folds, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        data_list.append([name, round(cv_results.mean(), 2), round(cv_results.std(),2)])\n",
    "    \n",
    "    # create a dataframe for the results\n",
    "    table = pd.DataFrame(np.array(data_list), columns=['MODEL NAME', 'RMSE', 'STD'])\n",
    "    table['RMSE'] = table['RMSE'].astype(float)\n",
    "    table = table.sort_values(by=['RMSE'], axis=0,ascending=False)\n",
    "    table.set_index('MODEL NAME', inplace=True)\n",
    "    \n",
    "    # convert dataframe to plotly table\n",
    "    fig = ff.create_table(table, colorscale=[[0, '#1F77B4'], [.5, '#ffffff'],[1, '#ffffff']], index=True)\n",
    "    fig.update_layout(width=400, plot_bgcolor='rgba(0,0,0,0)', height=300, margin=dict(l=0, r=10, t=10, b=10))\n",
    "    for i in range(len(fig.layout.annotations)):\n",
    "        fig.layout.annotations[i].font.size = 12\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be4144",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_linear = cross_val(10, 'neg_root_mean_squared_error')\n",
    "fig_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72f665",
   "metadata": {},
   "source": [
    "> From the dataframe above, we can see that `ARDRegressor`, `Ridge` and `BayesianRidge` have lowest `RMSE` values. Also, their standard deviations are approximately the same. Therefore, we will have to select the best model based on certain factors including RMSE, complexity and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ad54e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a040f",
   "metadata": {},
   "source": [
    "#### 5.4.2 Nonlinear Algorithm Evaluation<a id='nlae'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f743bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = []\n",
    "pipes.append(('KernelRidge', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", KernelRidge())])))\n",
    "pipes.append(('SVR', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", SVR())])))\n",
    "pipes.append(('KNeighbors', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", KNeighborsRegressor())])))\n",
    "pipes.append(('DTR', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", DecisionTreeRegressor())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e449f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_nlinear = cross_val(10, 'neg_root_mean_squared_error')\n",
    "fig_nlinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a5401",
   "metadata": {},
   "source": [
    "> For the non-linear models, `KernelRidge` performs better than all the other models and the standard deviation is similar to that of the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c1d9e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f00af",
   "metadata": {},
   "source": [
    "Finally, we will also evaluate ensemble algorithms. `Ensemble learning` is a general meta approach to machine learning that seeks better predictive performance by combining the predictions from multiple models.\n",
    "\n",
    "You can read more about ensemble learning [here](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b25e00",
   "metadata": {},
   "source": [
    "#### 5.4.3 Ensemble Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bedd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951eaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes=[]\n",
    "pipes.append(('RandomForest', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", RandomForestRegressor())])))\n",
    "pipes.append(('ExtraTrees', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", ExtraTreesRegressor())])))\n",
    "pipes.append(('AdaBR', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", AdaBoostRegressor())])))\n",
    "pipes.append(('GBR', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", GradientBoostingRegressor())])))\n",
    "pipes.append(('XGBRegressor', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", XGBRegressor())])))\n",
    "pipes.append(('LGBMRegressor', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", LGBMRegressor())])))\n",
    "pipes.append(('BaggingRegressor', Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", BaggingRegressor())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_en = cross_val(10, 'neg_root_mean_squared_error')\n",
    "fig_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1957cd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbd888",
   "metadata": {},
   "source": [
    "> The `RMSE` outputs from ensemble algorithms are quite high as compared to linear algorithms. To choose the final model, we have to look at the error margin, standard deviation and complexity. Looking at the algorithms, we will likely select <b>Ridge Regressor</b>. `Ridge Regressor` provides a good tradeoff between RMSE, and standard deviation. Its also quite simple to understand and less complex. Therefore, the best model to select will be `Ridge Regressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9863e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd8a0a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c03257",
   "metadata": {},
   "source": [
    "### 5.5 Hyperparameter Tuning<a id='pt'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe020fe",
   "metadata": {},
   "source": [
    "After determining the best algorithm to use based on the performance metric, stability and complexity, then the next step is search for the best hyperparameters that yields the best result. This is called `hyperparameter tuning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa15681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipelines and grid search parameters\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", Ridge())])\n",
    "tune_grid = {\"regressor__alpha\": np.linspace(0, 0.2, 21), \"regressor__fit_intercept\": [True, False], \n",
    "             \"regressor__normalize\": [True, False]}\n",
    "grid = GridSearchCV(pipe, param_grid=tune_grid, scoring='neg_mean_absolute_error', cv=12)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b8c4e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3373f6d",
   "metadata": {},
   "source": [
    "> Now we have seen a significant reduction in the RMSE after hyperparameter tuning. We will go ahead and build the final build and visualize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73419444",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944d63b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a78e00",
   "metadata": {},
   "source": [
    "### 5.6 Finalizing Model<a id='fm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583de33",
   "metadata": {},
   "source": [
    "In this section we will finalize the model by training it on the entire training dataset and make predictions for the hold-out test dataset to confirm our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.01, fit_intercept=True, normalize=False)\n",
    "linear_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", model)])\n",
    "linear_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b63b0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c4027",
   "metadata": {},
   "source": [
    "Now we will go ahead and make inference on the test data and then visualize how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f40d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16394423",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE: {}'.format(metrics.mean_absolute_error(y_test, predictions)))\n",
    "print('MSE: {}'.format(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('RMSE: {}'.format(np.sqrt(metrics.mean_squared_error(y_test, predictions))))\n",
    "print('R2: {}'.format(metrics.r2_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349dc1d4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6afa5d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e518c87",
   "metadata": {},
   "source": [
    "### 5.7 Model Understanding<a id='mdu'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0981c",
   "metadata": {},
   "source": [
    "In this final step, we will look at how well the model performed on the test data and then understand why the model made those predictions. For this step we will use both `sckit-learn evalutions metrics` as well as `yellowbrick visuals` for detailed understanding.\n",
    "\n",
    "`Yellowbrick library` is excellent for visualizing various ML tasks including hyperparameter tuning, model evaluation, model performance etc. You can explore more of the library's functionalities [here](https://www.scikit-yb.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c929844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "from yellowbrick.regressor import ManualAlphaSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a5643",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c651b9a",
   "metadata": {},
   "source": [
    "Now we will go ahead and visualize model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d82692",
   "metadata": {},
   "source": [
    "#### Residual and Prediction Error Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f05a7",
   "metadata": {},
   "source": [
    "A [residual plot](https://www.scikit-yb.org/en/latest/api/regressor/residuals.html) shows the difference between the observed value of the target variable (y) and the predicted value (ŷ), i.e. the error of the prediction. We will go ahead and visualize how our Ridge regressor performed. The [prediction error plot](https://www.scikit-yb.org/en/latest/api/regressor/peplot.html) shows the actual targets from the dataset against the predicted values generated by our model. This allows us to see how much variance is in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(12,5))\n",
    "\n",
    "# residual plot\n",
    "visualizer = ResidualsPlot(linear_pipe, ax=axes[0])\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test) \n",
    "visualizer.finalize()\n",
    "\n",
    "# predicted error plot\n",
    "visualizer = PredictionError(linear_pipe, ax=axes[1])\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.finalize()\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0ec9e",
   "metadata": {},
   "source": [
    "> Here we can see that points are randomly dispersed around the horizontal axis which clearly indicates that a linear model was the right call. In addition, the Ridge Regressor is performing well and we observe a Test R squared value of `90.4%`. Also, we can see from the histogram that our error is approximately normally distributed around zero, which also generally indicates our model is fitted well as we can see from the prediction error plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cea08",
   "metadata": {},
   "source": [
    "#### Alpha Selection and Cooks Distance Plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e0924",
   "metadata": {},
   "source": [
    "Regularization is designed to penalize model complexity, therefore the higher the alpha, the less complex the model, decreasing the error due to variance (overfit). In order to determine the `sweet spot` such that our model doesn't overfit due to high alpha values, we plot the alpha selection point to determine the optimal alpha value. The reason we're plotting this graph is to help us determine if our model is `overfitting` or not. \n",
    "\n",
    "[Cook’s Distance](https://www.scikit-yb.org/en/latest/api/regressor/influence.html) is a measure of an observation or instances’ influence on a linear regression. Datasets with a large number of highly influential points might not be suitable for linear regression without further processing such as outlier removal or imputation. \n",
    "\n",
    "Lets go ahead and plot this two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593671d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation on train data\n",
    "\n",
    "transformed_X = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2779cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(12,5))\n",
    "\n",
    "# alpha selection plot\n",
    "alphas = np.linspace(0, 0.2, 21)\n",
    "visualizer = ManualAlphaSelection(Ridge(), alphas=alphas, cv=12, scoring=\"neg_mean_squared_error\", ax=axes[0])\n",
    "visualizer.fit(transformed_X, y_train)\n",
    "visualizer.finalize()\n",
    "\n",
    "# Instantiate and fit the visualizer\n",
    "visualizer = CooksDistance(ax=axes[1])\n",
    "visualizer.fit(transformed_X, y_train)\n",
    "visualizer.finalize()\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05c84b",
   "metadata": {},
   "source": [
    "> From the alpha error plot, we can see that the error decreases until it reaches the `sweet spot (optimal alpha value)`. As the alpha value increases, the model tends to overfit and can clearly see an increase in the error after the sweet spot. The right alpha value is `0.01` which is what we selected in training our model thus we're confident that our model is not overfitting.\n",
    "\n",
    "> Looking at the cook's distance, we can see that we've just one highly influential plot indicating less outliers in our dataset. Therefore, selecting the Ridge regressor was a good option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b94a2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb4b15",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f140f11",
   "metadata": {},
   "source": [
    "### 5.8 Save Model<a id='sm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c12dbf",
   "metadata": {},
   "source": [
    "The final step after evaluating our model and checking the performance is to save model to be used for deployment. Now we will go ahead and save our model to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to a pickle file\n",
    "\n",
    "with open('final_model', 'wb') as file:\n",
    "    pickle.dump(linear_pipe, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7227b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b59992",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b5334",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2357f0",
   "metadata": {},
   "source": [
    "### 6. Conclusion and Recommendation<a id='cr'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef98e9",
   "metadata": {},
   "source": [
    "Before the beginning of this project, we set out to achieve 2 main goals based on the dataset we had available. The goals were;\n",
    "* Segment dataset and draw unique insights, including visualization of the transaction volume and assessing the effect of any outliers.\n",
    "* Explore correlation between customer attributes and build a regression and a decision-tree prediction model based on your findings. \n",
    "\n",
    "At the end of this project, we were able to draw unique insights from the dataset by answering about 6 questions. Some of the insights generated were:\n",
    "* Most of the transactions were performed by customers who were relatively young and between the ages of `18-40yrs`\n",
    "* We also found that the average transaction amount was about `29 AUD`\n",
    "* Also, `52.2%` of the overall transactions were completed by males as compared to `48%` for females.\n",
    "* Out of the percentage stated above, most of transactions per gender were debit transactions which clearly shows most customers are spending than earning. However, amounts were greater for credit transactions.\n",
    "* Again, out of the overall number of transactions, only `5000` were actually posted, in order words completed whereas the rest were authorized and still waiting for funds to be deducted from account.\n",
    "* Male spenders spend approximately `500 AUD` more than their female counterparts. To buttress that, the statistical analysis we performed showed that males were big spenders as compared to females. As a result, we concluded with `95%` confidence that average spend for males will mostly fall between `28.2 AUD` and `29.2 AUD` and average female spend would fall between `26.4 AUD` and `27.4 AUD`\n",
    "* We also found out that `Sydney` and `Melbourne` are the top two suburbs where debit transactions take place a lot. Also, New South Wale was found to be the state where spending was highest.\n",
    "* We also discovered that, spending decreases slightly towards the end of the month.\n",
    "* Finally, further investigation is needed to be carried out as to why there was not transaction on `August 16, 2018`.\n",
    "\n",
    "To add, we went ahead to evaluate various algorithms and finally selected the `ridge regressor` algorithm which was used to build a model that was able to explain approximately `90%` of the variation in the dataset.\n",
    "\n",
    "Despite the high coefficient of determination (R squared) value for salary prediction we had a very high `RMSE` value (8661) which could be as a result of the fact that the data we used was not that rich (large). Therefore, to reduce the RMSE we recommend we train the model with a more data and determine which features will greatly impact our model performance through feature selection techniques. \n",
    "\n",
    "In conclusion, we can confidently say that we have achieved our 2 main goals and have also tested our inital hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e263f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c330e3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ef38d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f612f99",
   "metadata": {},
   "source": [
    "### 7. References<a id='r'></a>\n",
    "\n",
    "[Move Up](#mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fcad6",
   "metadata": {},
   "source": [
    "* [Scikit Learn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n",
    "* [Machine Learning Mastery](https://machinelearningmastery.com/extra-trees-ensemble-with-python/)\n",
    "* [Yellowbrick Library](https://www.scikit-yb.org/en/latest/)\n",
    "* [Data Science Infinity DS Templates](https://data-science-infinity.teachable.com/)\n",
    "* [Data Science Blog](https://www.reneshbedre.com/blog/anova.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fb1ff",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ec6fd",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
